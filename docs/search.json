[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallel Computing Practical",
    "section": "",
    "text": "https://github.com/Phillip-Liu123/Peer-Reviewed-Assessment-2\n\n0.0.1 Question 1\n\nlibrary(foreach)\n\nnum_reps &lt;- 100\n\nresults &lt;- foreach(i = 1:num_reps, .combine = rbind) %do% {\n  # Generate 100 random samples from Exp(1)\n  sample_data &lt;- rexp(100, rate = 1)\n  # Calculate mean\n  sample_mean &lt;- mean(sample_data) \n  # Calculate variance\n  sample_variance &lt;- var(sample_data) \n  \n  c(sample_mean, sample_variance)\n}\n\nmat &lt;- cbind(rep(\"Observation\", 100), 1:100)\n\nname_vector &lt;- apply(mat, 1, function(row_names) paste(row_names[1], row_names[2]))\n\n# convert results into a data frame\nresults_df &lt;- as.data.frame(results)\ncolnames(results_df) &lt;- c(\"Mean\", \"Variance\")\nrownames(results_df) &lt;- (name_vector)\n\n# Display the first few rows\nhead(results_df)\n\n                   Mean  Variance\nObservation 1 1.1028840 1.0888644\nObservation 2 0.9076827 0.7659991\nObservation 3 1.1463451 1.2633645\nObservation 4 0.9202660 0.7755185\nObservation 5 0.9939132 0.9966638\nObservation 6 1.0136972 0.9002559\n\n\n\n\n0.0.2 Question 2\n\nlibrary(MASS)\nlibrary(doParallel)\nlibrary(foreach)\n\n# Number of bootstrap samples\nB &lt;- 10000\ndata &lt;- galaxies\n\n# Step 1: Serial Bootstrapping\nserial_start &lt;- Sys.time()\n  \nserial_bootstrap &lt;- foreach(i = 1:B,.combine = 'c') %do% {\n  median(sample(data, replace = TRUE))\n}\n\nserial_end &lt;- Sys.time()\nserial_time &lt;- serial_end - serial_start\n\n# Step 2: Parallel Bootstrapping (Single Samples per Iteration)\n# Set up parallel backend\nnum_cores &lt;- detectCores() - 1 \ncl &lt;- makeCluster(num_cores)\nregisterDoParallel(cl)\n\nparallel_start &lt;- Sys.time()\n\nparallel_bootstrap &lt;- foreach(i = 1:B, .combine = c) %dopar% {\n  median(sample(data, replace = TRUE))\n}\n\nparallel_end &lt;- Sys.time()\nparallel_time &lt;- parallel_end - parallel_start\n\nstopCluster(cl)\n\n# Step 3: Parallel Bootstrapping (Larger Chunks of 1000 Samples per Iteration)\n\ncl &lt;- makeCluster(num_cores)\nregisterDoParallel(cl)\n\nchunk_size &lt;- 1000\nnum_chunks &lt;- B / chunk_size # Number of iterations with chunking\n\nchunked_parallel_start &lt;- Sys.time()\n\nchunked_parallel_bootstrap &lt;- foreach(i = 1:num_chunks, .combine = c) %dopar% {\n  replicate(chunk_size, median(sample(data, replace = TRUE)))\n}\n\nchunked_parallel_end &lt;- Sys.time()\nchunked_parallel_time &lt;- chunked_parallel_end - chunked_parallel_start\n\nstopCluster(cl)\n\ncat(\"\\nPerformance Comparison:\\n\")\n\n\nPerformance Comparison:\n\ncat(\"Serial Processing Time:\", serial_time, \"\\n\")\n\nSerial Processing Time: 1.053923 \n\ncat(\"Parallel Processing Time (Single Samples per Iteration):\", parallel_time, \"\\n\")\n\nParallel Processing Time (Single Samples per Iteration): 1.365163 \n\ncat(\"Parallel Processing Time (Chunks of 1000):\", chunked_parallel_time, \"\\n\")\n\nParallel Processing Time (Chunks of 1000): 0.07534504 \n\n\n\n\n0.0.3 Question 3\n\nlibrary(foreach)\nlibrary(doParallel)\n\ncl &lt;- makeCluster(num_cores)\nregisterDoParallel(cl)\n\nB &lt;- 1000\nO &lt;- 1000  # Number of original samples\ntrue_mean &lt;- 1\n\nos_samp &lt;- rexp(50, rate = 1)\n\ncoverage &lt;- foreach(i = 1:O, .combine = c, .packages = \"foreach\") %dopar% {\n  \n  os_samp &lt;- rexp(50,rate = 1)\n  \n  boot_means &lt;- foreach(i = 1:B, .combine = c) %do% {\n    boot_sample &lt;- sample(os_samp, 50, replace = TRUE)\n    mean(boot_sample)\n  }\n  \n  ci &lt;- quantile(boot_means, c(0.025,0.975))\n  \n  (ci[1] &lt; true_mean) & (ci[2] &gt; true_mean)\n}\n\nmean(coverage)\n\n[1] 0.937\n\nstopCluster(cl)\n\n\n\n0.0.4 Question 4\n\n# Load required libraries\nlibrary(iterators)\n\ncl &lt;- makeCluster(num_cores)\nregisterDoParallel(cl)\n\n# Set the seed for reproducibility\nset.seed(1234)\n\n# Use foreach to iterate over each vector, compute the maximum, and print the results\nmax_values &lt;- foreach(i = 1:3, .combine = c, .packages = \"iterators\") %dopar% {\n  it &lt;- irnorm(1, n = 5)\n  max(nextElem(it))\n}\n\n# Print the maximum values\nprint(max_values)\n\n[1] 1.774299 1.960345 2.421811\n\nstopCluster(cl)\n\n\n\n0.0.5 Question 5\n\n# Load necessary libraries\nlibrary(parallel)\nlibrary(foreach)\nlibrary(doParallel)\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Method 1: replicate (serial)\ntime_replicate &lt;- system.time({\n  result_replicate &lt;- replicate(3, {\n    vec &lt;- irnorm(5)\n    max(nextElem(vec))\n  })\n})\n\n# Method 2: parLapply (parallel)\ntime_parLapply &lt;- system.time({\n  cl &lt;- makeCluster(detectCores()) # Create cluster\n  clusterSetRNGStream(cl, 1234) # Set seed for reproducibility\n  result_parLapply &lt;- parLapply(cl, 1:3, function(x) {\n    library(foreach)\n    library(iterators)\n    vec &lt;- irnorm(5) # Generate 5 random numbers\n    max(nextElem(vec)) # Compute maximum\n  })                                \n  stopCluster(cl)                   \n})\n\n# Method 3: foreach (parallel)\ntime_foreach &lt;- system.time({\n  cl &lt;- makeCluster(detectCores())\n  registerDoParallel(cl)\n  clusterSetRNGStream(cl, 1234)\n  result_foreach &lt;- foreach(i = 1:3, .combine = c) %dopar% {\n    library(foreach)\n    library(iterators)\n    vec &lt;- irnorm(5)\n    max(nextElem(vec))\n  }\n  stopCluster(cl)\n})\n\n# Print results\ncat(\"replicate time:\\n\")\n\nreplicate time:\n\nprint(time_replicate)\n\n   user  system elapsed \n  0.002   0.000   0.001 \n\ncat(\"\\nparLapply time:\\n\")\n\n\nparLapply time:\n\nprint(time_parLapply)\n\n   user  system elapsed \n  0.009   0.008   0.218 \n\ncat(\"\\nforeach time:\\n\")\n\n\nforeach time:\n\nprint(time_foreach)\n\n   user  system elapsed \n  0.010   0.005   0.214 \n\n\nThe observed results, where the replicate function outperformed both parLapply and foreach in terms of speed, align perfectly with theoretical expectations for small-scale tasks like the one tested. This behavior occurs because parallel processing inherently introduces overhead from cluster initialisation, task distribution, and result aggregation. For trivial computations—such as generating three vectors of five random numbers each and finding their maxima—the time spent managing parallel workers (starting processes, transferring data, synchronising results) far exceeds the time required to perform the actual calculations. Serial methods like replicate avoid this overhead entirely, executing tasks sequentially without the logistical costs of parallelisation. This explains why replicate completed the task in a fraction of the time (ie: ~0.001 seconds) compared to the parallel methods (~1.5 seconds), which spent most of their runtime on setup rather than computation.\nThe results also highlight a critical principle of parallel computing: the benefits of parallelisation depend heavily on the relationship between task size and overhead. Parallel frameworks like parLapply and foreach excel in large-scale scenarios—such as bootstrapping thousands of samples or running complex simulations—where the computational workload per iteration is substantial enough to justify the overhead. Conversely, for tiny tasks like this example, the overhead dominates, making serial methods more efficient. The minor timing differences between parLapply and foreach reflect implementation nuances but do not alter the overarching conclusion.\nThese findings underscore the importance of evaluating problem size before choosing a parallel approach. While parallel processing is powerful for scaling up intensive workloads, it becomes counterproductive for small, quick tasks. To see its advantages in action, one could rerun the experiment with larger parameters (ie: 10,000 iterations or longer vectors), where parallel methods would likely overtake replicate in efficiency. Ultimately, the results reinforce the rule that parallelisation should be reserved for problems where the computational effort per iteration outweighs the inherent costs of parallel management.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Parallel Computing Practical</span>"
    ]
  }
]